%! Author = borisdeletic
%! Date = 09/05/2023

% Preamble
\documentclass[11pt]{article}

% Document
\begin{document}

\section{Hamiltonian Monte Carlo}\label{sec:hamiltonian_monte_carlo}
    Hamiltonian Monte Carlo (HMC) is a powerful Markov-Chain Monte-Carlo (MCMC) method used to effectively generate
    random samples from a probability distribution.
    HMC was originally developed for Lattice QCD~\cite{HMC_Duane}, but has since found wide applications in statistical
    physics, computational biology, and machine learning.
    The most significant advantage of HMC is its ability to sample from very high dimensional ($D$ > millions)
    complex distributions, which is currently unmatched by other MCMC algorithms.

\subsection{Metropolis-Hastings Algorithm}\label{subsec:metropolis_hastings}
    HMC is a subclass of the Metropolis-Hastings algorithm~\cite{Metropolis_OG}, a MCMC which comprises of two
    steps: a proposal and a correction.
    Given an initial state $\theta$, a new sample $\theta'$ is proposed using some stochastic process.
    The proposal is then corrected to ensure detailed-balance so the Markov-Chain converges to the
    target distribution $\pi(\theta)$.
    The correction is performed by choosing to keep the new sample with an acceptance probability
    \begin{equation}\label{eq:metropolis_hastings}
        A(\theta' | \theta) = \min \left(1, \frac{Q(\theta | \theta') \pi(\theta') }{Q(\theta' | \theta) \pi(\theta) } \right),
    \end{equation}
    where $Q(\theta | \theta')$ is the probability distribution of the proposal function.
    All methods we will explore have symmetric proposal distributions $Q(\theta | \theta') = Q(\theta; | \theta)$,
    such that the acceptance probability reduces to
    \begin{equation}\label{eq:metropolis}
        A(\theta' | \theta) = \min \left(1, \frac{\pi(\theta') }{\pi(\theta) } \right).
    \end{equation}

    With a set of $N$ samples generated by Metropolis-Hastings, $\{ \theta_i \}$, we can estimate expected values of observables
    of the target distribution as
    \begin{equation}\label{eq:mcmc_observable}
    \begin{aligned}
        \langle \mathcal{O} \rangle = \frac{1}{N}\sum_{i=0}^N \mathcal{O}(\theta_i).
    \end{aligned}
    \end{equation}

\subsection{Hamiltonian Dynamics}\label{subsec:hamiltonian_dynamics}
    Hamiltonian dynamics is a formulation in classical physics based on the principle of least action.
    It generalises a system to its position $q$ and momenta $p$ in order to solve problems by working in \emph{phase space}.

    The Hamiltonian fully characterises the evolution of a system and is defined as the sum of kinetic ($T$) and
    potential ($U$) energy
    \begin{equation}\label{eq:hamiltonian_definition}
    \begin{aligned}
        H = T + U.
    \end{aligned}
    \end{equation}

    After obtaining the Hamiltonian, one uses the principle of least action and the Euler-Lagrange equations to solve for
    the equations of motion
    \begin{equation}\label{eq:hamiltonian_motion}
    \begin{aligned}
        \frac{dq}{dt} = \frac{\partial H}{\partial p}, \quad \frac{dp}{dt} = -\frac{\partial H}{\partial q}.
    \end{aligned}
    \end{equation}

    For a classical particle with mass $m$ moving through a static potential, the Hamiltonian is given by
    \begin{equation}\label{eq:classic_hamiltonian}
    \begin{aligned}
        H = \frac{p^2}{2m} + U(q).
    \end{aligned}
    \end{equation}
    Given initial conditions $(q_0, p_0)$, we integrate the equations of motion~\ref{eq:hamiltonian_motion} to find
    the trajectory of the particle.

\subsection{HMC Algorithm}\label{subsec:hmc_algorithm}
    HMC derives its strengths from incorporating the use of Hamiltonian dynamics to generate new samples in the Markov-Chain.
    The main idea is to treat the parameters $\theta$ as a position and introduce an auxiliary momentum variable $p$.
    Then, treating the target distribution as a potential energy, we can navigate the landscape by solving Hamilton's
    equations~\ref{eq:hamiltonian_motion} and integrating the trajectory from $(\theta_0, p_0)$ to a new point
    $(\theta_1, p_1)$.
    Finally, a correction step is performed as described~\ref{eq:metropolis_hastings} and the new sample is accepted
    with a given probability.

    Suppose we are trying to sample from a target distribution $\pi(\theta)$, the Hamiltonian for HMC is defined as
    \begin{equation}\label{eq:hmc_hamiltonian}
    \begin{aligned}
        H = \frac{1}{2} \mathbf{p}^T M^{-1} \mathbf{p} - \log \pi,
    \end{aligned}
    \end{equation}
    where $M$ is a mass matrix known as the \emph{metric}, and $\mathbf{p}$ is a new auxiliary momentum variable we have introduced.
    Intuitively, this treats the sample as a $D$ dimensional particle moving through a potential well $U = -\log \pi(\theta)$.

    For each Hamiltonian, the momentum is stochastically drawn from a normal distribution with
    standard deviation $M$, $\mathbf{p} \sim \mathcal{N}(0, \sigma = M)$.
    The metric therefore defines the region of phase space HMC explores and is an important input parameter to
    tune for effective sampling exploration~\cite{betancourt2016energymetric}.

    Once the Hamiltonian is obtained, the equations of motion~\ref{eq:hamiltonian_motion} are numerically integrated
    with step size $\epsilon$ and a fixed number of steps $L$ to give a new
    sample $(\theta_0, p_0)$ \rightarrow $(\theta_1, p_1)$.
    In order to solve the equations of motion we therefore rely on the \emph{gradient} of the
    distribution $\nabla \pi(\theta)$.
    This often presents a practical difficult as not many distributions have easily accessible gradients, however this
    can be alleviated using \emph{auto differentiation} as discussed further in~\ref{sec:autodiff}~\cite{carpenter2015stan}.

    The Hamiltonian satisfies the principle of energy conservation, so theoretically $H$ is unchanged during
    the evolution $H(\theta_0, q_0)) = H(\theta_1, q_1))$.
    However, the use of numerical integration introduces an energy drift.
    To counter this, a Metropolis correction step is performed to choose whether to accept or reject the sample in
    the Markov-Chain with probability
    \begin{equation}\label{eq:hmc_accept_prob}
    \begin{aligned}
        A(\theta' | \theta) = \min \left(1, \frac{\exp(-H(\theta_1, q_1)) }{\exp(-H(\theta_0, q_0)) } \right).
    \end{aligned}
    \end{equation}

    HMC naturally preserves detailed balance due to the principle of time-reversibility.
    Under T-symmetry where all momenta are reversed $\mathbf{p} \rightarrow -\mathbf{p}$, the Hamiltonian is
    time-reversible.
    Therefore, the probability of generating any two points is symmetric $(\theta_0, q_0) \leftrightarrow (\theta_1, q_1)$.

\subsection{HMC Parameters}\label{subsec:hmc_params}
    The three main input parameters for HMC are the step size $\epsilon$, path length $L$, and metric $M$.
    It is essential to properly tune these parameters for each distribution to ensure functioning sampling.

    The step size $\epsilon$ affects the accuracy of numerical integration and therefore the change in energy between the initial
    and final point $\Delta H = H(\theta_1, q_1)) - H(\theta_0, q_0))$.
    Choosing $\epsilon$ to be too large will result in high rejection rates during the correction step, causing
    inefficient sampling in the Markov-Chain, while an $\epsilon$ too small will lead to longer integration
    times $L\epsilon$ wasting computational resources.

    The path length $L$ with $\epsilon$ defines the integration time $L\epsilon$ of the numerical integration.
    It is important for the integration time to be sufficient such that the proposal point is sufficiently decorrelated
    from the initial point.

    Considering the Hamiltonian from an energy perspective $H(E)$, the metric defines the distribution of energy level
    sets which HMC explores.
    It is important for the metric to be properly chosen such that the momenta drawn are not too large or small and
    adequately explore the target distribution level sets.

\subsection{Sampling within an Iso-Likelihood Contour}\label{subsec:isolikelood_sampling}
    Nested sampling requires that samples are generated subject to the hard likelihood constraint
    $\mathcal{L} > \mathcal{L}_i$ at each iteration.
    It is a challenging problem to draw points from the constrained distribution, with current approaches such as
    slice sampling~\cite{neal2003slice} and rejection sampling~\cite{Feroz_2009} scaling poorly with dimension.

    HMC samples from the unconstrained distribution $\pi(\theta)$.
    In order to leverage its use in nested sampling, it is therefore necessary to modify the algorithm to sample
    given a constraint.
\end{document}