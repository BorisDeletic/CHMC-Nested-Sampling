%! Author = borisdeletic
%! Date = 08/05/2023

% Preamble
\documentclass[11pt]{article}

% Document
\begin{document}

    \section{Bayesian Inference}\label{sec:bayesian_inference}
    Bayesian inference is a robust analytic framework which allows the construction of predictive models $\mathcal{M}$
    in the context of some dataset $\mathcal{D}$.

    The \emph{likelihood} is defined as the probability of observing the data given a specific parameter choice $\theta$
    \begin{equation}\label{eq:likelihood}
        p(\mathcal{D} | \theta, \mathcal{M}) \equiv \mathcal{L}(\theta).
    \end{equation}
    A Bayesian model must also specify it's distribution of parameters before any data is known.
    This is termed the \emph{prior}, defined by
    \begin{equation}\label{eq:prior}
        p(\theta|\mathcal{M}) \equiv \pi(\theta).
    \end{equation}
    The \emph{evidence} is the distribution of observed data marginalised over the parameters, defined as
    \begin{equation}\label{eq:evidence}
        p(\mathcal{D} | \mathcal{M}) \equiv \mathcal{Z} = \int{\mathcal{L}(\theta) \pi(\theta) d\theta}.
    \end{equation}
    The evidence, or sometimes \emph{marginalised likelihood}, is an important quantity which provides a measure
    on the quality of a model $\mathcal{M}$.

    By using Bayes' Theorem~\cite{bishop2006}, the distribution of parameters $\theta$ given our model and data
    can be written in terms of the quantities
    \begin{equation}\label{eq:bayes_theorem}
    \begin{aligned}
        p(\theta | \mathcal{D}, \mathcal{M}) &=
            \frac{p(\mathcal{D} | \theta, \mathcal{M}) p(\theta|\mathcal{M})}{p(\mathcal{D} | \mathcal{M})}, \\
        \mathcal{P}(\theta) &= \frac{\mathcal{L}(\theta) \pi(\theta)}{\mathcal{Z}},
    \end{aligned}
    \end{equation}
    where $\mathcal{P}(\theta)$ is termed the \emph{posterior}.
    The posterior is the distribution of parameters $\theta$ after taking the data into account.

    An intuitive relationship between these quantities is that a model where the \emph{prior} more closely
    resembles the \emph{posterior} will have a greater \emph{evidence}~\cite{mackay2003}.

    Calculating the posterior is in the domain of \emph{parameter estimation}, which is often a very difficult task
    to solve analytically.
    For high-dimensional problems, we therefore resort to computationally estimating the distribution by sampling
    from the posterior using Markov-Chain Monte-Carlo techniques~\cite{gupta2014comparison, delmoral2013mean}.

    Examples of such sampling algorithms include Metropolis-Hastings~\cite{Metropolis_OG},
    Slice sampling~\cite{neal2003slice}, and Hamiltonian Monte Carlo~\cite{HMC_Duane, neal1996monte} which
    we explore further in section~\ref{sec:hamiltonian_monte_carlo}.

    Single chain-based sampling algorithms struggle with multi-modal distributions due to topological freezing.
    When one sample from the chain falls into a topological feature, it can take a very long time before it escapes and
    continues to explore the space.
    This motivates the use of multiple points in parameter space to perform sampling, as introduced by nested sampling.
\end{document}